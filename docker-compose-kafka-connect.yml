version: '3.9'

services:

  nginx:
    image: nginx:1.22.1
    container_name: nginx
    environment:
      TZ: $TZ
    ports:
      - 8083:80
    depends_on:
        kafka-connect-01:
          condition: service_started
        kafka-connect-02:
          condition: service_started
        kafka-connect-03:
          condition: service_started
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/conf.d/default.conf

  # https://hub.docker.com/r/confluentinc/cp-kafka-connect/tags
  kafka-connect-01:
    image: confluentinc/cp-kafka-connect:${CONFLUENT_PLATFORM_VERSION}
    container_name: kafka-connect-01
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    ports:
      - 18083:8083
    environment:
      # Configuration properties for Kafka Connect workers (which apply to all connectors run)
      CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: "[%d] %p %X{connector.context}%m (%c:%L)%n"
      CONNECT_BOOTSTRAP_SERVERS: "kafka:29092"
      CONNECT_REST_PORT: 8083
      CONNECT_CLIENT_ID: "kafka-connect-01"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect-01"
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: kafka-connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: kafka-connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: kafka-connect-status
      # https://docs.confluent.io/platform/current/connect/security.html#externalizing-secrets
      CONNECT_CONFIG_PROVIDERS: "file"
      CONNECT_CONFIG_PROVIDERS_FILE_CLASS: "org.apache.kafka.common.config.provider.FileConfigProvider"
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_LOG4J_ROOT_LOGLEVEL: "INFO"
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"
      # Since we are running a single broker Kafka cluster we can't replicate data. So the need to configure
      # the replication factor to 1!
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_PLUGIN_PATH: '/usr/share/java'
      TZ: $TZ
    healthcheck:
      interval: 10s
      retries: 20
      test: curl --fail --silent http://localhost:8083/ --output /dev/null || exit 1
    volumes:
      - ./connectors/confluentinc-kafka-connect-datagen-0.6.0:/usr/share/java/confluentinc-kafka-connect-datagen
      - ./connectors/kafka-connect-transforms:/usr/share/java/kafka-connect-transforms
      - ./avro/StockQuote.avsc:/kafka-connect/datagen/avro/StockQuote.avsc

  # https://hub.docker.com/r/confluentinc/cp-kafka-connect/tags
  kafka-connect-02:
    image: confluentinc/cp-kafka-connect:${CONFLUENT_PLATFORM_VERSION}
    container_name: kafka-connect-02
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    ports:
      - 28083:8083
    environment:
      # Configuration properties for Kafka Connect workers (which apply to all connectors run)
      CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: "[%d] %p %X{connector.context}%m (%c:%L)%n"
      CONNECT_BOOTSTRAP_SERVERS: "kafka:29092"
      CONNECT_REST_PORT: 8083
      CONNECT_CLIENT_ID: "kafka-connect-02"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect-02"
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: kafka-connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: kafka-connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: kafka-connect-status
      # https://docs.confluent.io/platform/current/connect/security.html#externalizing-secrets
      CONNECT_CONFIG_PROVIDERS: "file"
      CONNECT_CONFIG_PROVIDERS_FILE_CLASS: "org.apache.kafka.common.config.provider.FileConfigProvider"
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_LOG4J_ROOT_LOGLEVEL: "INFO"
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"
      # Since we are running a single broker Kafka cluster we can't replicate data. So the need to configure
      # the replication factor to 1!
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_PLUGIN_PATH: '/usr/share/java'
      TZ: $TZ
    healthcheck:
      interval: 10s
      retries: 20
      test: curl --fail --silent http://localhost:8083/ --output /dev/null || exit 1
    volumes:
      - ./connectors/confluentinc-kafka-connect-datagen-0.6.0:/usr/share/java/confluentinc-kafka-connect-datagen
      - ./connectors/kafka-connect-transforms:/usr/share/java/kafka-connect-transforms
      - ./avro/StockQuote.avsc:/kafka-connect/datagen/avro/StockQuote.avsc

  # https://hub.docker.com/r/confluentinc/cp-kafka-connect/tags
  kafka-connect-03:
    image: confluentinc/cp-kafka-connect:${CONFLUENT_PLATFORM_VERSION}
    container_name: kafka-connect-03
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    ports:
      - 38083:8083
    environment:
      # Configuration properties for Kafka Connect workers (which apply to all connectors run)
      CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: "[%d] %p %X{connector.context}%m (%c:%L)%n"
      CONNECT_BOOTSTRAP_SERVERS: "kafka:29092"
      CONNECT_REST_PORT: 8083
      CONNECT_CLIENT_ID: "kafka-connect-03"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect-03"
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: kafka-connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: kafka-connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: kafka-connect-status
      # https://docs.confluent.io/platform/current/connect/security.html#externalizing-secrets
      CONNECT_CONFIG_PROVIDERS: "file"
      CONNECT_CONFIG_PROVIDERS_FILE_CLASS: "org.apache.kafka.common.config.provider.FileConfigProvider"
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_LOG4J_ROOT_LOGLEVEL: "INFO"
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"
      # Since we are running a single broker Kafka cluster we can't replicate data. So the need to configure
      # the replication factor to 1!
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_PLUGIN_PATH: '/usr/share/java'
      TZ: $TZ
    healthcheck:
      interval: 10s
      retries: 20
      test: curl --fail --silent http://localhost:8083/ --output /dev/null || exit 1
    volumes:
      - ./connectors/confluentinc-kafka-connect-datagen-0.6.0:/usr/share/java/confluentinc-kafka-connect-datagen
      - ./connectors/kafka-connect-transforms:/usr/share/java/kafka-connect-transforms
      - ./avro/StockQuote.avsc:/kafka-connect/datagen/avro/StockQuote.avsc

  conduktor:
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
      kafka-connect-01:
        condition: service_healthy
      kafka-connect-02:
        condition: service_healthy
      kafka-connect-03:
        condition: service_healthy

volumes:
  conduktor_data: {}